---
title: "Titanic Survival Rate (Kaggle)"
author: "Keh-Harng Feng"
date: "June 21, 2017"
header-includes:
    - \usepackage{placeins}
output: 
  bookdown::html_document2:
    fig_caption: TRUE
    toc: FALSE
urlcolor: blue
---

```{r setup, include=FALSE}
library('knitr')
library('lattice')
library('caret')
library('missForest')
library('parallel')
library('doParallel')


opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, tidy = TRUE, cache = TRUE)
options(digits = 4)

## functions

# Data PreProcessing
# Function that checks if the measurement has no missing values for useful features.

chk_missing <- function(datapoint, useful) {
    useful_vars <- names(datapoint)[names(datapoint) %in% useful]
    
    return(sum(is.na(datapoint[useful_vars])) > 0)
}


# Remove data with missing values and return preprocessed data, chosen indices 
# and original data.
my_preProcess <- function(data, useful) {
    n <- nrow(data)
    
    good_ind <- rep(FALSE, n)
    
    for (i in 1:n) {
        good_ind[i] = !chk_missing(data[i,], useful)
    }
    
    ans = list(data = data[good_ind,], ind = good_ind, original = data)
    
    return(ans)
}


# Data imputation
my_impute <- function(data, useful) {
    imputed_vars <- missForest(data[,names(data) %in% useful], ntree = 5001, 
                               variablewise = FALSE)
    
    data[,names(data) %in% useful] <- imputed_vars$ximp
    
    ans = list(data.imp = data, obj.imp = imputed_vars)
    return(ans)
}

my_predict <- function(train.model, data, useful) {
    # Preprocess testing set.
    data.preProc <- my_preProcess(data, useful)
    
    pred <- rep(NA, nrow(data))
    
    pred[data.preProc$ind] <- as.numeric(as.character(predict(train.model, 
                                                              data.preProc$data)))
    pred <- factor(pred)
    
    return(pred)
}
```

# Data Ingress & Preprocesss
The Titanic survival data is stored in csv format and imported into R using `read.csv()`. The first couple rows of the resulting data frame are shown below:

```{r}
data <- read.csv('train.csv', header = TRUE, na.string = '')
n <- nrow(data)
head(data)
```

The following variables are identified as categorical and converted to factors:
```{r}
# Categorical variables: Sex, Survived, Pclass, Embarked
categorical <- c('Sex', 'Survived', 'Pclass', 'Embarked', 'Name', 'Cabin', 'PassengerId', 'Ticket')

for (var in categorical) {
    data[,var] <- factor(data[,var])
}

print(categorical)
```

Notice that `Survived` is the response, therefore this is a classification problem. There are `r n` observations so the sample is a decent size. The training data is thus further split into a 80% training and 20% validation set for model building.

```{r}
# 80%, 20% training/validation split
set.seed(123)
inTrain <- sample(1:n, size = ceiling(n*0.8))

data.train <- data[inTrain, ]
data.validation <- data[-inTrain, ]
```

## Feature Selection
From this point on all operations are carried out on the 80% training set only unless otherwise specified. Feature selection is done mainly by logic. Essentially, predictors that I do not believe to have a logical connection to the survival rate are excluded from the model. These are shown below

```{r}
# Uselss features: Name, Cabin, PassengerId
useless <- c('Name', 'Cabin', 'PassengerId', 'Ticket')

cabin_unknown <- sum(is.na(data$Cabin))
```

It should be noted that `Cabin` as a predictor can potential predict a passenger's location on the ship. Therefore it is possible for it to have a logical connection to survival rate. However, the data as it stands has over `r cabin_unknown` missing entries (out of `r n` total entries) for the `Cabin` predictor. With so many missing entries it is impossible to for it be imputed reliably. `Cabin` is therefore discarded.

For the numerical predictors a check for colinearity is carried out using `caret::findCorrelation()` with the cutoff set at 0.9:

```{r}
# All potentially useful predictors
useful <- names(data.train)[!(names(data.train) %in% 
                                             c(useless, 'Survived'))]

# Useful numerical predictors
useful.numerical <- useful[!(useful %in% categorical)]

# check if any predictors should be removed due to colinearity
print(findCorrelation(x = cor(data.train[,useful.numerical]), names = TRUE, 
                cutoff = 0.9))
```

There are no colinear predictors. With this in mind the preliminary features selected for model training are:

```{r}
print(useful)
```

The reason the port of initial embarkation (`Embarked`) is included is because it may be connected to where the passenger's room was located on the ship.

## Data Imputation
```{r}
n_miss <- sum(is.na(data.train$Age))
```
The `Age` predictor is the only selected predictor that contains missing data. The percentage of missing data is `r n_miss/nrow(data.train)`. Since the predictors selected are a mix of numerical and categorical, the imputation method must be able to deal with factors. Random forest is selected to impute the missing data (set at 5001 trees to avoid ties). OOB imputation error estimate is shown below:

```{r}
num_trees <- 5001

set.seed(123)
imputation <- my_impute(data.train, useful)

print(imputation$obj.imp$OOBerror)

data.train.imp <- imputation$data

age_error <- imputation$obj.imp$OOBerror[1]*mean(data.train.imp$Age)
```

With a normalized MSE of `r imputation$obj.imp$OOBerror[1]` the estimated OOB error for imputed age is `r age_error`.

# Model Building
Model is first built using random forest on the training set with the number of features selected at each level (`mtry`) tuned from 1 to all of the useful predictors at once using 10-fold CV.

```{r}
formula <- as.formula(paste('Survived', paste(useful, collapse = ' + '), 
                            sep = ' ~ '))

tunegrid <- expand.grid(mtry = c(1:length(useful)))

trCon <- trainControl(method = 'cv', number = 10, search = 'grid',
                      allowParallel = TRUE)

# Train Random Forest model
cl <- makePSOCKcluster(detectCores(logical = FALSE))
registerDoParallel(cl)

set.seed(123)
trCon <- trainControl(method = 'cv', number = 10, search = 'grid', 
                      allowParallel = TRUE)
rf_gridsearch.imp <- train(formula, data = data.train.imp, method = 'rf', 
                       ntree = num_trees, tuneGrid = tunegrid, 
                       importance = TRUE, na.action = na.fail, trControl = trCon) 

stopCluster(cl)
```

```{r}
rf_gridsearch.imp
```

Figure \@ref(fig:varimp) shows the variable importance from the selected features. It seems that the most important predictor is `Sex` by far - a whopping 250 misclassifications out of `r n` observations if `Sex` is permuted.
```{r varimp, fig.cap = 'Variable importance for the optimal random forest model on the training set.'}
varImpPlot(rf_gridsearch.imp$finalModel)
```

The final model built from the 80% training set is used to make predictions on the validation set. The confusion matrix is shown below:

```{r}
set.seed(123)
validation.imputation <- my_impute(data.validation, useful)

print(validation.imputation$obj.imp$OOBerror)

data.validation.imp <- validation.imputation$data

pred.imp <- predict(rf_gridsearch.imp, data.validation.imp)

confMat <- confusionMatrix(pred.imp, data.validation.imp$Survived)

confMat
```


# Production Model & Test Set Prediction
The production model is trained on the entire training set (again with 10-CV tuning on `mtry` and 5001 trees).

```{r, results = 'hide'}
data.imp <- my_impute(data, useful)$data

cl <- makePSOCKcluster(detectCores(logical = FALSE))
registerDoParallel(cl)

set.seed(123)
rf_production.imp <- train(formula, data = data.imp, method = 'rf', 
                       ntree = num_trees, tuneGrid = tunegrid, 
                       importance = TRUE, na.action = na.fail, trControl = trCon) 

stopCluster(cl)
```
The test set is similarly imputed and predicted using the production model.

```{r}
data.test <- read.csv('test.csv', header = TRUE, na.string = '')

for (var in categorical[-2]) {
    data.test[,var] <- factor(data.test[,var])
}


test.imputation <- my_impute(data.test, useful)
```

```{r}
data.test.imp <- test.imputation$data

test.pred <- predict(rf_production.imp, data.test.imp)

df.out <- data.frame(PassengerId = data.test.imp$PassengerId, Survived = test.pred)

write.csv(df.out, file = 'my_submission.csv', row.names = FALSE, quote = FALSE)
```